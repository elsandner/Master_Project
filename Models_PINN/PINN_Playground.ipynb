{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 7ms/step - loss: 0.1097 - val_loss: 0.0874\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.0830\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 0.0820\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0849 - val_loss: 0.0817\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0858 - val_loss: 0.0814\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0847 - val_loss: 0.0820\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0850 - val_loss: 0.0817\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0842 - val_loss: 0.0811\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0838 - val_loss: 0.0813\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0839 - val_loss: 0.0806\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1553ba850>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "timesteps = 10\n",
    "input_dim = 1\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(num_samples, timesteps, input_dim)\n",
    "y_NDBC = np.random.rand(num_samples, 1)\n",
    "y_ERA5 = np.random.rand(num_samples, 1)\n",
    "\n",
    "# Your custom loss function\n",
    "def combined_mse(alpha=0.5):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true_1, y_true_2 = tf.split(y_true, num_or_size_splits=2, axis=1)\n",
    "\n",
    "        mse_1 = tf.reduce_mean(tf.square(y_pred - y_true_1))\n",
    "        mse_2 = tf.reduce_mean(tf.square(y_pred - y_true_2))\n",
    "        return alpha * mse_1 + (1 - alpha) * mse_2\n",
    "    return loss\n",
    "\n",
    "# Define your LSTM model using the Sequential API\n",
    "neurons = 50\n",
    "batch_size = 25\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "y_combined = np.concatenate([y_NDBC, y_ERA5], axis=1)\n",
    "model.compile(optimizer='adam', loss=combined_mse(alpha=0.5))\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "split_index = int(num_samples * 0.8)\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y_combined[:split_index], y_combined[split_index:]\n",
    "\n",
    "# Train your model\n",
    "epochs = 10\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Master_Project/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Master_Project/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:906\u001B[0m, in \u001B[0;36mTensorShape.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    905\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_v2_behavior:\n\u001B[0;32m--> 906\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dims\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    907\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    908\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdims[key]\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "model.predict(10, batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Austins Code:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def penalized_loss_fn(lam):\n",
    "    def loss(y_true, y_pred):\n",
    "\n",
    "        \"\"\"The y_true term contains the regularizor HyCOM step on the second axis.\n",
    "        This is not directly generated by the model, so we slice the term off the end\n",
    "        and simply use it as a regularization term to the buoy forecast.\n",
    "\n",
    "        lam = 0 is no real buoy data used, all HyCOM regularization\n",
    "        lam = 1 is all real buoy data, no HyCOM regularization\"\"\"\n",
    "\n",
    "        print(y_true.shape, y_pred.shape)\n",
    "\n",
    "        water_temp_true = y_true[:,0]\n",
    "        hy_water_temp_true = y_true[:,3]\n",
    "\n",
    "        water_delta1 = y_pred[:,0] - water_temp_true\n",
    "        water_delta2 = y_pred[:,0] - hy_water_temp_true\n",
    "\n",
    "        gust_temp_true = y_true[:,1]\n",
    "        c_gust_temp_true = y_true[:,11]\n",
    "\n",
    "        gust_delta1 = y_pred[:,1] - gust_temp_true\n",
    "        gust_delta2 = y_pred[:,1] - c_gust_temp_true\n",
    "\n",
    "        pres_temp_true = y_true[:,2]\n",
    "        c_pres_temp_true = y_true[:,15]\n",
    "\n",
    "        pres_delta1 = y_pred[:,2] - pres_temp_true\n",
    "        pres_delta2 = y_pred[:,2] - c_pres_temp_true\n",
    "\n",
    "\n",
    "        model_pred = tf.math.reduce_mean(keras.backend.abs(y_pred[:,3:]-y_true[:,3:]))\n",
    "\n",
    "        temp_out =  tf.math.reduce_mean( tf.math.scalar_mul(lam, keras.backend.abs(water_delta1))\n",
    "                                  + tf.math.scalar_mul((1-lam), keras.backend.abs(water_delta2)))\n",
    "\n",
    "        gust_out = tf.math.reduce_mean( tf.math.scalar_mul(lam, keras.backend.abs(gust_delta1))\n",
    "                                  + tf.math.scalar_mul((1-lam), keras.backend.abs(gust_delta2)))\n",
    "\n",
    "        pres_out = tf.math.reduce_mean( tf.math.scalar_mul(lam, keras.backend.abs(pres_delta1))\n",
    "                                  + tf.math.scalar_mul((1-lam), keras.backend.abs(pres_delta2)))\n",
    "\n",
    "        return temp_out+gust_out+pres_out+model_pred\n",
    "\n",
    "    return loss\n",
    "\n",
    "def build_1D_gru(lam, window_size, num_channels, deep_layers):\n",
    "\n",
    "    input = layers.Input(shape=(num_channels, window_size))\n",
    "\n",
    "    x = layers.Reshape(target_shape=(window_size,num_channels))(input)\n",
    "    x = layers.Dense(256, activation='tanh')(x)\n",
    "    x = layers.BatchNormalization(scale=True, center=True, momentum=0.999) (x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.GRU(256, activation='tanh', return_sequences=True)(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.GRU(256, activation='tanh', return_sequences=True)(x)\n",
    "\n",
    "\n",
    "    for d in range(deep_layers):\n",
    "        x = layers.Dense(256, activation='tanh')(x)\n",
    "        x = layers.BatchNormalization(scale=True, center=True, momentum=0.999) (x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.GRU(256, activation='tanh', return_sequences=True)(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "\n",
    "        if (d == deep_layers-1):\n",
    "            x = layers.GRU(256, activation='tanh', return_sequences=False)(x)\n",
    "\n",
    "        else:\n",
    "            x = layers.GRU(256, activation='tanh', return_sequences=True)(x)\n",
    "\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(200, activation='tanh')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(200, activation='tanh')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(200, activation='tanh')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(200, activation='tanh')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(num_channels, activation='linear')(x)\n",
    "\n",
    "    model = Model(input, x)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=[penalized_loss_fn(lam)])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val):\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH,\n",
    "                    shuffle=False,\n",
    "                    validation_data=(x_val, y_val),)# verbose=0)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def display_history(history, model, x_test, y_test):\n",
    "    #res = model.evaluate(x=x_test,y=y_test)\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Model Evaluated Training Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "def get_predictions(model, x_test):\n",
    "    predictions = model.predict(x=x_test,verbose = 0)\n",
    "    return predictions\n",
    "\n",
    "def get_multi_predictions(model, x_test, horizon_size):\n",
    "    predictions=[]\n",
    "    for i in range(horizon_size):\n",
    "        x_test = model.predict(x=x_test[:,1:],verbose = 0)\n",
    "        predictions.append(x_test)\n",
    "    return np.array(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}