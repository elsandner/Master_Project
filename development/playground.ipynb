{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TEST PLAYGROUND\n",
    "#Just for dev puproses!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Austins code for loss function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def penalized_loss_fn(lam):\n",
    "    def loss(y_true, y_pred):\n",
    "\n",
    "        \"\"\"The y_true term contains the regularizor HyCOM step on the second axis.\n",
    "        This is not directly generated by the model, so we slice the term off the end\n",
    "        and simply use it as a regularization term to the buoy forecast.\n",
    "\n",
    "        lam = 0 is no real buoy data used, all HyCOM regularization\n",
    "        lam = 1 is all real buoy data, no HyCOM regularization\"\"\"\n",
    "\n",
    "        print(y_true.shape, y_pred.shape)\n",
    "\n",
    "        water_temp_true = y_true[:,0]\n",
    "        hy_water_temp_true = y_true[:,3]\n",
    "\n",
    "        water_delta1 = y_pred[:,0] - water_temp_true\n",
    "        water_delta2 = y_pred[:,0] - hy_water_temp_true\n",
    "\n",
    "        gust_temp_true = y_true[:,1]\n",
    "        c_gust_temp_true = y_true[:,11]\n",
    "\n",
    "        gust_delta1 = y_pred[:,1] - gust_temp_true\n",
    "        gust_delta2 = y_pred[:,1] - c_gust_temp_true\n",
    "\n",
    "        pres_temp_true = y_true[:,2]\n",
    "        c_pres_temp_true = y_true[:,15]\n",
    "\n",
    "        pres_delta1 = y_pred[:,2] - pres_temp_true\n",
    "        pres_delta2 = y_pred[:,2] - c_pres_temp_true\n",
    "\n",
    "\n",
    "        model_pred = tf.math.reduce_mean(keras.backend.abs(y_pred[:,3:]-y_true[:,3:]))\n",
    "\n",
    "        temp_out =  tf.math.reduce_mean( tf.math.scalar_mul(lam, keras.backend.abs(water_delta1))\n",
    "                                  + tf.math.scalar_mul((1-lam), keras.backend.abs(water_delta2)))\n",
    "\n",
    "        gust_out = tf.math.reduce_mean( tf.math.scalar_mul(lam, keras.backend.abs(gust_delta1))\n",
    "                                  + tf.math.scalar_mul((1-lam), keras.backend.abs(gust_delta2)))\n",
    "\n",
    "        pres_out = tf.math.reduce_mean( tf.math.scalar_mul(lam, keras.backend.abs(pres_delta1))\n",
    "                                  + tf.math.scalar_mul((1-lam), keras.backend.abs(pres_delta2)))\n",
    "\n",
    "        return temp_out+gust_out+pres_out+model_pred\n",
    "\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Austins code for Transformer Model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "lam = alpha"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 1, 18)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1, 512)            9728      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  (None, 1, 512)           11016692  \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1, 512)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1, 512)            2099200   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 1, 512)            0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1, 512)            262656    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 1, 512)            0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1, 512)           2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1, 200)            102600    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1, 200)            0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1, 200)            40200     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 1, 200)            0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1, 200)            40200     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 1, 200)            0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1, 200)            40200     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 1, 200)            0         \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 18)                3618      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,619,190\n",
      "Trainable params: 13,617,142\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.05):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                             key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"selu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)  # self-attention layer\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)  # layer norm\n",
    "        ffn_output = self.ffn(out1)  #feed-forward layer\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)  # layer norm\n",
    "\n",
    "def build_attention_model(lam, num_channels=18, deep_layers=4):\n",
    "    #embed_dim = n_features  # Embedding size for each token\n",
    "    embed_dim = 512\n",
    "    num_heads = 10  # Number of attention heads\n",
    "    ff_dim = 500  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Reshape(target_shape=(1,num_channels)))\n",
    "    model.add(layers.Dense(512) )\n",
    "    model.add(layers.BatchNormalization( scale=True, center=True, momentum=0.999) )\n",
    "\n",
    "    for a in range(deep_layers):\n",
    "        model.add(TransformerBlock(embed_dim, num_heads, ff_dim, rate=0.05))\n",
    "        model.add(layers.Dropout(0.05))\n",
    "        model.add(layers.LSTM(512, return_sequences=True))\n",
    "        model.add(layers.Dropout(0.05))\n",
    "        model.add(layers.Dense(512) )\n",
    "        model.add(layers.Dropout(0.05))\n",
    "        model.add( layers.BatchNormalization( scale=True, center=True, momentum=0.999) )\n",
    "\n",
    "    for a in range(4):\n",
    "            model.add(layers.Dense(200, ))\n",
    "            model.add(layers.Dropout(0.05))\n",
    "\n",
    "    model.add(keras.layers.Flatten(name='Flatten'))\n",
    "\n",
    "    model.add(layers.Dense(num_channels, activation='linear'))\n",
    "\n",
    "    learning_rate = 1e-6\n",
    "    decay_rate = 1e-10\n",
    "    momentum = 0.9\n",
    "\n",
    "    sgd = keras.optimizers.legacy.Adam(learning_rate=learning_rate,  decay=decay_rate, ) #stochastic gradient decent ?!\n",
    "\n",
    "    model.compile(loss=penalized_loss_fn(lam), optimizer=sgd)\n",
    "\n",
    "    return model\n",
    "\n",
    "model=build_attention_model(lam=0.4,\n",
    "                            num_channels=18,\n",
    "                            deep_layers=1\n",
    "                            )\n",
    "\n",
    "model.build((None,18,1)) # `input_shape` is the shape of the input data\n",
    "                         # e.g. input_shape = (None, 32, 32, 3)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}