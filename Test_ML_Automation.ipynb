{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Test Automation\n",
    "\n",
    "Runs Test_ML_Models_A in a loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import myLibrary as mL\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "NDBC = mL.NDBC_lib\n",
    "ERA5 = mL.ERA5_lib\n",
    "Models = mL.Models\n",
    "DP = mL.DataProcessor\n",
    "Experiment = mL.Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def run_test_A(MODEL_NAME, ALPHA, filename, report_description,\n",
    "               DATAFILE = \"dataset_GOM_1_A_A.pickle\", STATIONARY_SHIFT = 1, N_TEST_HOURS=24):\n",
    "\n",
    "    #Read data from file\n",
    "    data_directory = os.path.join(os.getcwd(), f'data/datasets/type_A')\n",
    "    with open(f'data/datasets/type_A/{DATAFILE}', 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    data = dataset[\"data\"]\n",
    "\n",
    "    #Preprocessing\n",
    "    data_stationary = DP.data_to_stationary(data, n = STATIONARY_SHIFT)\n",
    "    data_supervised = DP.data_to_supervised(data_stationary, n_in=3)\n",
    "\n",
    "    train_X, train_y, test_X, test_y = DP.train_test_split(data_supervised, N_TEST_HOURS)\n",
    "\n",
    "    #Training\n",
    "    start_time = time.time()\n",
    "    model = Models.get_model(MODEL_NAME, train_X, train_y, ALPHA)\n",
    "    TRAINING_TIME = time.time() - start_time\n",
    "\n",
    "    #One-Shot-Forecasting\n",
    "    model.predict(train_X, batch_size=1)\n",
    "    yhat = model.predict(test_X)\n",
    "\n",
    "    #Create Evaluation Dataframes\n",
    "    output_cols = data.columns.tolist()\n",
    "    yhat_df = pd.DataFrame(yhat, columns=[name + \"_pred\" for name in output_cols])\n",
    "    yhat_df.set_index(data.tail(len(yhat)).index, inplace=True)\n",
    "\n",
    "    evaluation_1 = data.tail(len(yhat)+1).copy()  #+1 since i need that value for de-differencing\n",
    "\n",
    "    #De-Differenciating\n",
    "    for col in evaluation_1.columns:\n",
    "        evaluation_1[f\"{col}_pred\"]= evaluation_1[col].shift(STATIONARY_SHIFT) + yhat_df[f\"{col}_pred\"]\n",
    "\n",
    "    evaluation_1 = evaluation_1.iloc[STATIONARY_SHIFT:]  # remove first n entries since there is no delta value for them\n",
    "\n",
    "    # Correct wind direction (modulo 360)\n",
    "    wdir_columns = [col for col in evaluation_1.columns if col.startswith(\"WDIR\")]\n",
    "    evaluation_1[wdir_columns] = evaluation_1[wdir_columns] % 360\n",
    "\n",
    "    #CREATE REPORT\n",
    "    # Convert model summary to string\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    model_summary = \"\\n\".join(stringlist)\n",
    "\n",
    "    report = Experiment(\n",
    "        name=filename,\n",
    "        description=report_description,\n",
    "\n",
    "        stations = dataset[\"stations\"],\n",
    "        years = dataset[\"years\"],\n",
    "        nan_threshold=dataset[\"nan_threshold\"],\n",
    "        features=dataset[\"features\"],\n",
    "        era5=dataset[\"add_era5\"],\n",
    "\n",
    "        stationary_shift=STATIONARY_SHIFT,\n",
    "\n",
    "        n_test_hours=N_TEST_HOURS,\n",
    "\n",
    "        #stationary=STATIONARY,\n",
    "        scaler= None, # SCALER,\n",
    "\n",
    "        model_name = MODEL_NAME,\n",
    "        model_summary=model_summary,\n",
    "        training_time = TRAINING_TIME,\n",
    "\n",
    "        one_shot_forecast = evaluation_1,\n",
    "        recursive_forecast = None   # evaluation_2\n",
    "    )\n",
    "\n",
    "\n",
    "    # open a file for writing in binary mode\n",
    "    filepath = f'data/reports/{report.name}.pickle'\n",
    "    with open(filepath, 'wb') as f:\n",
    "        # write the object to the file using pickle.dump()\n",
    "        pickle.dump(report, f)\n",
    "        print(f\"File successfully saved:{filepath}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def run_test_B(MODEL_NAME, ALPHA, filename, report_description,\n",
    "               DATAFILE = \"dataset_GOM_1_B_B.pickle\", STATIONARY_SHIFT = 1, N_TEST_HOURS=24):\n",
    "\n",
    "    #Read data from file\n",
    "    with open(f'data/datasets/type_B/{DATAFILE}', 'rb') as f:\n",
    "        # load the object from the file using pickle.load()\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    train = dataset[\"data_train\"]\n",
    "    test = dataset[\"data_test\"]\n",
    "\n",
    "    #Preprocessing\n",
    "    train_stationary = DP.data_to_stationary(train, n = STATIONARY_SHIFT)\n",
    "    test_stationary = DP.data_to_stationary(test, n = STATIONARY_SHIFT)\n",
    "\n",
    "    #Data is already supervised!\n",
    "\n",
    "    train_X, train_y, _, _ = DP.train_test_split(train_stationary, -len(train_stationary))\n",
    "    _, _, test_X, test_y = DP.train_test_split(test_stationary, len(test_stationary))\n",
    "\n",
    "    #Training\n",
    "    start_time = time.time()\n",
    "    model = Models.get_model(MODEL_NAME, train_X, train_y, ALPHA)\n",
    "    TRAINING_TIME = time.time() - start_time\n",
    "\n",
    "    #One-Shot-Forecasting\n",
    "    model.predict(train_X, batch_size=1)\n",
    "    yhat = model.predict(test_X)\n",
    "\n",
    "    #Create Evaluation Dataframes\n",
    "    output_cols  = test.loc[:, ~test.columns.str.contains('t-')].columns\n",
    "\n",
    "    yhat_df = pd.DataFrame(yhat)\n",
    "    yhat_df.columns = [name + \"_pred\" for name in output_cols]\n",
    "    yhat_df.set_index(test.tail(len(yhat)).index, inplace=True)\n",
    "    evaluation_1 = test.loc[:, ~test.columns.str.contains('t-')]    #ground truth: just y, without X\n",
    "\n",
    "    #De-Differenciating\n",
    "    for col in evaluation_1.columns:\n",
    "        shifted = evaluation_1[col].shift(STATIONARY_SHIFT)\n",
    "        evaluation_1[f\"{col}_pred\"]= shifted + yhat_df[f\"{col}_pred\"]\n",
    "\n",
    "    evaluation_1 = evaluation_1.iloc[STATIONARY_SHIFT:]  # remove first n entries since there is no delta value for them\n",
    "\n",
    "    # Correct wind direction (modulo 360)\n",
    "    wdir_columns = [col for col in evaluation_1.columns if col.startswith(\"WDIR\")]\n",
    "    evaluation_1[wdir_columns] = evaluation_1[wdir_columns] % 360\n",
    "\n",
    "    #CREATE REPORT\n",
    "    #Convert model summary to string\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    model_summary = \"\\n\".join(stringlist)\n",
    "\n",
    "    report = Experiment(\n",
    "            name=filename,\n",
    "            description=report_description,\n",
    "\n",
    "            stations = dataset[\"files\"],\n",
    "            years = [\"not available\"],\n",
    "            nan_threshold=dataset[\"nan_threshold\"],\n",
    "            features=dataset[\"features\"],\n",
    "            era5=dataset[\"add_era5\"],\n",
    "\n",
    "            stationary_shift=STATIONARY_SHIFT,\n",
    "            # lag=1,\n",
    "            n_test_hours=dataset[\"num_test_hours\"],\n",
    "\n",
    "            #stationary=STATIONARY,\n",
    "            scaler= None, # SCALER,\n",
    "\n",
    "            model_name = MODEL_NAME,\n",
    "            model_summary=model_summary,\n",
    "            training_time = TRAINING_TIME,\n",
    "\n",
    "            one_shot_forecast = evaluation_1,\n",
    "            recursive_forecast = None\n",
    "    )\n",
    "\n",
    "    # open a file for writing in binary mode\n",
    "    filepath = f'data/reports/{filename}.pickle'\n",
    "    with open(filepath, 'wb') as f:\n",
    "        # write the object to the file using pickle.dump()\n",
    "        pickle.dump(report, f)\n",
    "        print(\"File successfully saved:\")\n",
    "        print(filepath)\n",
    "\n",
    "\n",
    "    #Release Data Variables to reduce RAM usage\n",
    "    del train\n",
    "    del test\n",
    "    del train_stationary\n",
    "    del test_stationary\n",
    "    del train_X\n",
    "    del train_y\n",
    "    del test_X\n",
    "    del test_y\n",
    "    del model\n",
    "    del yhat\n",
    "    del stringlist\n",
    "    del model_summary\n",
    "    del filepath\n",
    "    del report\n",
    "    gc.collect()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "approach = \"SSUD\"   # \"MLM\" ... Multi Location Modelling, \"SSUD\" ... Station Specific Unified Dataset\n",
    "model_names = [\"TCN\"] # [\"GRU\", \"CNN\", \"TCN\"] #[\"LSTM\", \"GRU\", \"CNN\", \"TCN\"]\n",
    "alpha_values = np.arange(0.1, 1.1, 0.1)\n",
    "report_description = \"Executed with automated script. Corrected wind direction. excluded scaling.\"\n",
    "\n",
    "print(alpha_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 08:20:17.252767: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11712/11712 [==============================] - 175s 15ms/step - loss: 605.0357 - val_loss: 606.0220\n",
      "Epoch 2/100\n",
      "11712/11712 [==============================] - 152s 13ms/step - loss: 587.5862 - val_loss: 592.4197\n",
      "Epoch 3/100\n",
      "11712/11712 [==============================] - 152s 13ms/step - loss: 578.8094 - val_loss: 587.6688\n",
      "Epoch 4/100\n",
      "11712/11712 [==============================] - 152s 13ms/step - loss: 574.4376 - val_loss: 582.1690\n",
      "Epoch 5/100\n",
      "11712/11712 [==============================] - 152s 13ms/step - loss: 571.9647 - val_loss: 581.0779\n",
      "Epoch 6/100\n",
      "11712/11712 [==============================] - 153s 13ms/step - loss: 569.6220 - val_loss: 580.0752\n",
      "Epoch 7/100\n",
      "11712/11712 [==============================] - 154s 13ms/step - loss: 568.2913 - val_loss: 581.8637\n",
      "Epoch 8/100\n",
      "11712/11712 [==============================] - 158s 14ms/step - loss: 566.4588 - val_loss: 580.1691\n",
      "Epoch 9/100\n",
      "11712/11712 [==============================] - 155s 13ms/step - loss: 565.2667 - val_loss: 579.6829\n",
      "Epoch 10/100\n",
      "11712/11712 [==============================] - 155s 13ms/step - loss: 563.9919 - val_loss: 579.4104\n",
      "Epoch 11/100\n",
      "11712/11712 [==============================] - 154s 13ms/step - loss: 562.6223 - val_loss: 577.5603\n",
      "Epoch 12/100\n",
      "11712/11712 [==============================] - 154s 13ms/step - loss: 562.1888 - val_loss: 579.3374\n",
      "Epoch 13/100\n",
      "11712/11712 [==============================] - 155s 13ms/step - loss: 560.6673 - val_loss: 580.3376\n",
      "Epoch 14/100\n",
      "11712/11712 [==============================] - 154s 13ms/step - loss: 560.1270 - val_loss: 578.1478\n",
      "Epoch 15/100\n",
      "11712/11712 [==============================] - 154s 13ms/step - loss: 558.7567 - val_loss: 574.0637\n",
      "Epoch 16/100\n",
      "11712/11712 [==============================] - 153s 13ms/step - loss: 558.2717 - val_loss: 576.8630\n",
      "Epoch 17/100\n",
      "11712/11712 [==============================] - 336s 29ms/step - loss: 557.5391 - val_loss: 577.8630\n",
      "Epoch 18/100\n",
      "11712/11712 [==============================] - 156s 13ms/step - loss: 558.5671 - val_loss: 576.3413\n",
      "Epoch 19/100\n",
      "11712/11712 [==============================] - 158s 13ms/step - loss: 558.4587 - val_loss: 574.5833\n",
      "Epoch 20/100\n",
      "11712/11712 [==============================] - 158s 13ms/step - loss: 556.5153 - val_loss: 575.9794\n",
      "Epoch 21/100\n",
      "11712/11712 [==============================] - 157s 13ms/step - loss: 556.0796 - val_loss: 576.4145\n",
      "Epoch 22/100\n",
      "11712/11712 [==============================] - 156s 13ms/step - loss: 555.9185 - val_loss: 578.1066\n",
      "Epoch 23/100\n",
      "11712/11712 [==============================] - 156s 13ms/step - loss: 556.5334 - val_loss: 578.3831\n",
      "Epoch 24/100\n",
      "11712/11712 [==============================] - 155s 13ms/step - loss: 554.6021 - val_loss: 578.3995\n",
      "Epoch 25/100\n",
      "11712/11712 [==============================] - 159s 14ms/step - loss: 553.1614 - val_loss: 575.1959\n",
      "Epoch 26/100\n",
      "11712/11712 [==============================] - 157s 13ms/step - loss: 554.0763 - val_loss: 581.5778\n",
      "Epoch 27/100\n",
      "11712/11712 [==============================] - 173s 15ms/step - loss: 553.0668 - val_loss: 578.5328\n",
      "Epoch 28/100\n",
      "11712/11712 [==============================] - 177s 15ms/step - loss: 553.3167 - val_loss: 576.3541\n",
      "Epoch 29/100\n",
      "11712/11712 [==============================] - 178s 15ms/step - loss: 552.8470 - val_loss: 576.5209\n",
      "Epoch 30/100\n",
      "11712/11712 [==============================] - 179s 15ms/step - loss: 552.5991 - val_loss: 582.9333\n",
      "Epoch 31/100\n",
      "11712/11712 [==============================] - 191s 16ms/step - loss: 552.3490 - val_loss: 582.5002\n",
      "Epoch 32/100\n",
      "11712/11712 [==============================] - 181s 15ms/step - loss: 552.1591 - val_loss: 580.4615\n",
      "Epoch 33/100\n",
      "11712/11712 [==============================] - 178s 15ms/step - loss: 550.4340 - val_loss: 576.9674\n",
      "Epoch 34/100\n",
      "11712/11712 [==============================] - 181s 15ms/step - loss: 550.0800 - val_loss: 581.5276\n",
      "Epoch 35/100\n",
      " 7978/11712 [===================>..........] - ETA: 56s - loss: 537.4410"
     ]
    }
   ],
   "source": [
    "if approach == \"MLM\":\n",
    "    for model in model_names:\n",
    "        for alpha in alpha_values:\n",
    "            filename = f'report_A_{model}_{format(alpha, \".1f\")}'\n",
    "            run_test_A(model,alpha,filename,report_description)\n",
    "\n",
    "elif approach == \"SSUD\":\n",
    "    for model in model_names:\n",
    "        for alpha in alpha_values:\n",
    "            filename = f'report_B_{model}_{format(alpha, \".1f\")}'\n",
    "            run_test_B(model,alpha,filename,report_description)\n",
    "            print(filename)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}